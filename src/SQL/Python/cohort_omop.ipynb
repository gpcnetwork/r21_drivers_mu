{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import urllib.request as urlreq\n",
    "import os\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import col\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "path_to_data = os.path.join(os.getcwd(),'University of Missouri','DRIVERS','data')\n",
    "path_to_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"use real_world_data_ed_omop_dec_2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cd_meta = spark.sql('''\n",
    "    select concept_id,concept_name,concept_code,vocabulary_id,domain_id\n",
    "    from concept\n",
    "    where vocabulary_id = 'DRG' and\n",
    "          concept_code in (\n",
    "            '765','766','767','768',\n",
    "            '774','775',\n",
    "            '783','784','785','786', '787','788',\n",
    "            '796','797','798',\n",
    "            '805','806','807'\n",
    "          )\n",
    "    union all \n",
    "    select concept_id,concept_name,concept_code,vocabulary_id,domain_id\n",
    "    from concept\n",
    "    where vocabulary_id = 'CPT4' and\n",
    "          concept_code in (\n",
    "            '59409','59514', '59612','59620'\n",
    "          )\n",
    "    union all \n",
    "    select concept_id,concept_name,concept_code,vocabulary_id,domain_id\n",
    "    from concept\n",
    "    where vocabulary_id = 'ICD10PCS' and\n",
    "          concept_code in (\n",
    "            '10D00Z0','10D00Z1','10D00Z2','10D07Z3','10D07Z4', '10D07Z5', '10D07Z6','10D07Z7','10D07Z8',\n",
    "            '10E0XZZ'\n",
    "          )\n",
    "    union all\n",
    "    select concept_id,concept_name,concept_code,vocabulary_id,domain_id\n",
    "    from concept\n",
    "    where vocabulary_id = 'LOINC' and\n",
    "          concept_code in (\n",
    "            \n",
    "          )\n",
    "''').toPandas()\n",
    "cd_meta.to_csv(os.path.join(path_to_data,'cd_meta_omop.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def cd_where_clause(meta_tbl,voc_id):\n",
    "    '''\n",
    "    generate where clause details based on omop concept_id\n",
    "    '''\n",
    "    cd_lst = meta_tbl.loc[meta_tbl['vocabulary_id']==voc_id,'concept_id'].tolist()\n",
    "    cd_quote = []\n",
    "    for code in cd_lst:\n",
    "        cd_quote.append(\"'\"+ str(code) +\"'\")\n",
    "    cd_quote_str = \",\".join(cd_quote)\n",
    "    return cd_quote_str     \n",
    "\n",
    "meta_tbl = pd.read_csv(os.path.join(path_to_data,'cd_meta_omop.csv'))\n",
    "drg_where = cd_where_clause(meta_tbl,'DRG')\n",
    "cpt4_where = cd_where_clause(meta_tbl,'CPT4')\n",
    "icd10_where = cd_where_clause(meta_tbl,'ICD10PCS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "delivery_init = spark.sql('''\n",
    "    select person_id,\n",
    "           visit_occurrence_id, \n",
    "           visit_detail_id, \n",
    "           observation_date as event_date, \n",
    "           observation_concept_id as event_identifier,\n",
    "           'DRG' as event_source\n",
    "    from observation\n",
    "    where observation_concept_id in ('''+ drg_where +''')\n",
    "    union all\n",
    "    select person_id,\n",
    "           visit_occurrence_id, \n",
    "           visit_detail_id, \n",
    "           procedure_date as event_date, \n",
    "           procedure_concept_id as event_identifier,\n",
    "           'CPT4' as event_source\n",
    "    from procedure_occurrence\n",
    "    where procedure_concept_id in ('''+ cpt4_where +''')\n",
    "    union all\n",
    "    select person_id,\n",
    "           visit_occurrence_id, \n",
    "           visit_detail_id, \n",
    "           procedure_date as event_date, \n",
    "           procedure_concept_id as event_identifier,\n",
    "           'ICD10PCS' as event_source\n",
    "    from procedure_occurrence\n",
    "    where procedure_concept_id in ('''+ icd10_where +''')\n",
    "''').cache()\n",
    "delivery_init.createOrReplaceTempView(\"delivery_init\")\n",
    "delivery_init.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "    select event_source, count(distinct person_id)\n",
    "    from delivery_init\n",
    "    group by event_source \n",
    "''').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "delivery_ip = spark.sql('''\n",
    "    select distinct\n",
    "           a.person_id, \n",
    "           a.visit_occurrence_id,\n",
    "           v.visit_start_date,\n",
    "           v.visit_end_date,\n",
    "           v.care_site_id\n",
    "    from delivery_init a \n",
    "    join visit_occurrence v \n",
    "    on a.person_id = v.person_id and \n",
    "       a.visit_occurrence_id = v.visit_occurrence_id\n",
    "    where v.visit_concept_id in (\n",
    "        9201, -- IP\n",
    "        262   -- ER to IP\n",
    "    )\n",
    "''').cache()\n",
    "delivery_ip.createOrReplaceTempView(\"delivery_ip\")\n",
    "delivery_ip.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "delivery_consolidate = spark.sql('''\n",
    "    with cd_filter as (\n",
    "        select v.person_id, \n",
    "               v.visit_occurrence_id,\n",
    "               v.care_site_id,\n",
    "               v.visit_start_date,\n",
    "               v.visit_end_date,\n",
    "               a.event_source,\n",
    "               a.event_date,\n",
    "               row_number() over (partition by v.person_id, v.visit_occurrence_id, a.event_source order by a.event_date) as rn_asc,\n",
    "               row_number() over (partition by v.person_id, v.visit_occurrence_id, a.event_source order by a.event_date desc) as rn_desc\n",
    "        from delivery_ip v\n",
    "        join delivery_init a \n",
    "        on v.person_id = a.person_id and \n",
    "           v.visit_occurrence_id = a.visit_occurrence_id\n",
    "        where a.event_date between date_sub(v.visit_start_date,3) and date_add(v.visit_end_date,3)\n",
    "    ), f_pvt as (\n",
    "        select * \n",
    "        from (\n",
    "            select person_id, visit_occurrence_id,\n",
    "                   event_source, event_date\n",
    "            from cd_filter\n",
    "            where rn_asc = 1       \n",
    "        )\n",
    "        pivot (\n",
    "            min(event_date) for event_source in (\n",
    "                'DRG' as F_DRG_DT,'CPT4' as F_CPT_DT,'ICD10PCS' as F_ICD_DT\n",
    "            )\n",
    "        )\n",
    "    ), l_pvt as (\n",
    "        select * \n",
    "        from (\n",
    "            select person_id, visit_occurrence_id,\n",
    "                   event_source, event_date\n",
    "            from cd_filter\n",
    "            where rn_desc = 1       \n",
    "        )\n",
    "        pivot (\n",
    "            max(event_date) for event_source in (\n",
    "                'DRG' as L_DRG_DT,'CPT4' as L_CPT_DT,'ICD10PCS' as L_ICD_DT\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    select a.person_id, \n",
    "           a.visit_occurrence_id,\n",
    "           a.visit_start_date,\n",
    "           a.visit_end_date,\n",
    "           a.care_site_id,\n",
    "           f.F_DRG_DT,f.F_CPT_DT,f.F_ICD_DT,\n",
    "           l.L_DRG_DT,l.L_CPT_DT,l.L_ICD_DT\n",
    "    from delivery_ip a \n",
    "    left join f_pvt f on a.person_id = f.person_id and a.visit_occurrence_id = f.visit_occurrence_id\n",
    "    left join l_pvt l on a.person_id = l.person_id and a.visit_occurrence_id = l.visit_occurrence_id\n",
    "''').cache()\n",
    "delivery_consolidate.createOrReplaceTempView(\"delivery_consolidate\")\n",
    "delivery_consolidate.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "delivery_elig = spark.sql('''\n",
    "    with date_consolid as (\n",
    "        select distinct \n",
    "               person_id,\n",
    "               visit_occurrence_id,\n",
    "               care_site_id,\n",
    "               coalesce(F_DRG_DT,visit_start_date,F_ICD_DT,F_CPT_DT) as event_start_dt,\n",
    "               coalesce(L_DRG_DT,visit_end_date,L_ICD_DT,L_CPT_DT) as event_end_dt\n",
    "        from delivery_consolidate    \n",
    "    ), visit_diffs as (\n",
    "        select a.*, \n",
    "               lead(a.event_start_dt, 1, '9999-12-31') OVER (PARTITION BY person_id ORDER BY event_start_dt) AS next_event_start_dt\n",
    "        from date_consolid a \n",
    "    ), visit_session as (\n",
    "        select b.*, \n",
    "               case \n",
    "                   when datediff(b.next_event_start_dt,b.event_start_dt) > 211 then 1\n",
    "                   else 0 \n",
    "               end as new_session_flag\n",
    "        from visit_diffs b\n",
    "    ), sessions as (\n",
    "        select d.*, \n",
    "               sum(d.new_session_flag) over (PARTITION BY d.person_id ORDER BY d.event_start_dt) as event_id\n",
    "        from visit_session d\n",
    "    ), session_order as (\n",
    "        select e.*, \n",
    "               row_number() over (partition by e.person_id, e.event_id order by e.event_start_dt) as rn,\n",
    "               max(e.event_end_dt) over (partition by e.person_id, e.event_id) as event_end_date\n",
    "    from sessions e\n",
    "    )\n",
    "    select s.person_id, \n",
    "           s.event_id, \n",
    "           s.visit_occurrence_id,\n",
    "           cs.care_site_source_value,\n",
    "           s.event_start_dt as event_start_date,\n",
    "           s.event_end_date\n",
    "    from session_order s \n",
    "    join care_site cs on s.care_site_id = cs.care_site_id\n",
    "    where s.rn = 1\n",
    "    order by s.person_id, s.event_id\n",
    "''').cache()\n",
    "delivery_elig.createOrReplaceTempView(\"delivery_elig\")\n",
    "delivery_elig.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "delivery_elig_tbl1 = spark.sql('''\n",
    "    select d.person_id,\n",
    "           d.event_id,\n",
    "           d.event_start_date, \n",
    "           d.event_end_date,\n",
    "           coalesce(datediff(d.event_end_date,d.event_start_date),1) as los, \n",
    "           d.visit_occurrence_id,\n",
    "           p.year_of_birth,\n",
    "           year(d.event_start_date) - p.year_of_birth as age_at_event,\n",
    "           p.month_of_birth,\n",
    "           p.day_of_birth,\n",
    "           p.race_source_value,\n",
    "           p.ethnicity_source_value,\n",
    "           p.location_id,\n",
    "           p.care_site_id,\n",
    "           d.care_site_source_value,\n",
    "           tnt.bed_size,\n",
    "           tnt.speciality,\n",
    "           tnt.segment,\n",
    "           tnt.zip_code,\n",
    "           dth.death_date,\n",
    "           case when dth.death_date is not null then 1 else 0 end as death_ind\n",
    "    from delivery_elig d\n",
    "    join person p on d.person_id = p.person_id\n",
    "    left join tenant_attributes tnt on d.care_site_source_value = tnt.tenant\n",
    "    left join death dth on d.person_id = dth.person_id \n",
    "    where year(d.event_start_date) - p.year_of_birth between 10 and 55\n",
    "''').cache()\n",
    "delivery_elig_tbl1.createOrReplaceTempView(\"delivery_elig_tbl1\")\n",
    "delivery_elig_tbl1.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# load SMM code list and get omop concept_id\n",
    "json_url = urlreq.urlopen('https://raw.githubusercontent.com/RWD2E/phecdm/main/res/valueset_curated/vs-mmm.json')\n",
    "smm_json = json.loads(json_url.read())\n",
    "qry_lst = []\n",
    "def add_quote(lst):\n",
    "    lst_quote = [\"'\"+str(x)+\"'\" for x in lst]\n",
    "    return (lst_quote)\n",
    "for k,v in smm_json.items():\n",
    "    # exclude delivery codes\n",
    "    if k.startswith('d_'): continue\n",
    "    for cd,sig in v.items():\n",
    "        if cd=='long': continue\n",
    "        # entail the range\n",
    "        if 'range' in sig:\n",
    "            for x in sig['range']:\n",
    "                key_quote = [str(y) for y in list(range(int(x.split('-')[0]),int(x.split('-')[1])+1))]\n",
    "                sig['exact'].extend(key_quote)\n",
    "            \n",
    "        # generate dynamic queries\n",
    "        qry = '''\n",
    "            select ''' + \"'\" + k + \"'\" + ''' as SMM_GRP, \n",
    "                   ''' + \"'\" + v['long'] + \"'\" + ''' as SMM_GRP_LONG,\n",
    "                   concept_id,concept_name,concept_code,vocabulary_id,domain_id\n",
    "            from concept\n",
    "            where vocabulary_id = '''+ \"'\" + cd.upper() + \"'\" +''' and\n",
    "        '''\n",
    "        if 'icd' in cd and 'pcs' not in cd:\n",
    "            where_lev0 = '''substring_index(concept_code,'.',1) in ('''+ ','.join(add_quote(sig['lev0'])) +''')''' if sig['lev0'] else None\n",
    "            where_lev1 = '''substring(concept_code,1,5) in ('''+ ','.join(add_quote(sig['lev1'])) +''')''' if sig['lev1'] else None\n",
    "            where_lev2 = '''substring(concept_code,1,6) in ('''+ ','.join(add_quote(sig['lev2'])) +''')''' if sig['lev2'] else None\n",
    "            where_nonempty = [s for s in [where_lev0,where_lev1,where_lev2] if s is not None]\n",
    "            \n",
    "            qry += '''\n",
    "            (\n",
    "                 ''' + ' or '.join(where_nonempty) + '''  \n",
    "            )         \n",
    "            '''\n",
    "        else:\n",
    "            qry += '''\n",
    "            (\n",
    "                 concept_code in ('''+ ','.join(add_quote(sig['exact'])) +''')\n",
    "            )         \n",
    "            '''\n",
    "        qry_lst.append(qry)\n",
    "\n",
    "        \n",
    "# qry_final = ' union all '.join(qry_lst)\n",
    "# print(qry_final)\n",
    "# smm_omop_cd = spark.sql(' union all '.join(qry_lst)).toPandas()\n",
    "# smm_omop_cd.to_csv(os.path.join(path_to_data,'cd_meta_omop_smm.csv'),index=False)\n",
    "\n",
    "smm_omop_meta = spark.sql(' union all '.join(qry_lst)).cache()\n",
    "smm_omop_meta.createOrReplaceTempView(\"smm_omop_meta\")\n",
    "smm_omop_meta.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "smm_init = spark.sql('''\n",
    "    select px.person_id,\n",
    "           px.visit_occurrence_id, \n",
    "           px.procedure_date as event_date,\n",
    "           m.SMM_GRP\n",
    "    from procedure_occurrence px\n",
    "    join smm_omop_meta m\n",
    "    on px.procedure_concept_id = m.concept_id\n",
    "    where m.vocabulary_id in ('CPT4','HCPCS','ICD9PR','ICD10PCS')\n",
    "    union all\n",
    "    select person_id,\n",
    "           visit_occurrence_id, \n",
    "           condition_start_date as event_date,\n",
    "           m.SMM_GRP\n",
    "    from condition_occurrence dx\n",
    "    join smm_omop_meta m\n",
    "    on dx.condition_concept_id = m.concept_id\n",
    "    where m.vocabulary_id in ('ICD9CM','ICD10CM')\n",
    "''').cache()\n",
    "smm_init.createOrReplaceTempView(\"smm_init\")\n",
    "smm_init.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "smm_post_delivery = spark.sql('''\n",
    "    select a.person_id, b.event_id,\n",
    "           a.SMM_GRP,\n",
    "           b.event_start_date,a.event_date,\n",
    "           datediff(a.event_date,b.event_start_date) AS days_since_index\n",
    "    from smm_init a \n",
    "    join delivery_elig_tbl1 b \n",
    "    on a.person_id = b.person_id\n",
    "    where datediff(a.event_date,b.event_start_date) between 0 and 365\n",
    "''').cache()\n",
    "smm_post_delivery.createOrReplaceTempView(\"smm_post_delivery\")\n",
    "smm_post_delivery.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "smm_post_delivery_wide = spark.sql('''\n",
    "    select *\n",
    "    from (\n",
    "        select person_id, event_id, SMM_GRP,days_since_index\n",
    "        from smm_post_delivery\n",
    "     )\n",
    "    pivot \n",
    "    (\n",
    "        min(days_since_index) for SMM_GRP in (\n",
    "            'ami' as AMI_since_index,\n",
    "            'ane' as ANE_since_index,\n",
    "            'arf' as ARF_since_index,\n",
    "            'ards' as ARDS_since_index,\n",
    "            'afe' as AFE_since_index,\n",
    "            'cavf' as CAVF_since_index,\n",
    "            'cocr' as COCR_since_index,\n",
    "            'dic' as DIC_since_index,\n",
    "            'ecl' as ECL_since_index,\n",
    "            'hf' as HF_since_index,\n",
    "            'pcd' as PCD_since_index,\n",
    "            'pe' as PE_since_index,\n",
    "            'sac' as SAC_since_index,\n",
    "            'sep' as SEP_since_index,\n",
    "            'ssh' as SSH_since_index,\n",
    "            'scc' as SCC_since_index,\n",
    "            'ate' as ATE_since_index,\n",
    "            'bpt' as BPT_since_index,\n",
    "            'hys' as HYS_since_index,\n",
    "            'tt' as TT_since_index,\n",
    "            'ven' as VEN_since_index\n",
    "        )\n",
    "    )\n",
    "''').cache()\n",
    "smm_post_delivery_wide.createOrReplaceTempView(\"smm_post_delivery_wide\")\n",
    "smm_post_delivery_wide.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "delivery_elig_smm = spark.sql('''\n",
    "    with smm_any as (\n",
    "        select person_id, event_id, 1 as SMMANY_ind,\n",
    "               min(days_since_index) as SMMANY_since_index\n",
    "        from smm_post_delivery\n",
    "        group by person_id, event_id\n",
    "    )\n",
    "    select e.*,\n",
    "           case when e.age_at_event between 10 and 19 then 'agegrp1'\n",
    "                when e.age_at_event between 10 and 29 then 'agegrp2'\n",
    "                when e.age_at_event between 30 and 39 then 'agegrp3'\n",
    "                else 'agegrp4' \n",
    "           end as agegrp_at_event,\n",
    "           case when los >= 1 then 1 else 0 end as los1up_ind,\n",
    "           case when los >= 2 then 1 else 0 end as los2up_ind,\n",
    "           case when los >= 3 then 1 else 0 end as los3up_ind,\n",
    "           case when los >= 4 then 1 else 0 end as los4up_ind,\n",
    "           case when los >= 5 then 1 else 0 end as los5up_ind,\n",
    "           case when los >= 6 then 1 else 0 end as los6up_ind,\n",
    "           case when los >= 7 then 1 else 0 end as los7up_ind,           \n",
    "           a.SMMANY_since_index,\n",
    "           coalesce(a.SMMANY_ind,0) as SMMANY_ind,\n",
    "           s.AMI_since_index,\n",
    "           IF(s.AMI_since_index IS NOT NULL, 1, 0) AMI_ind,\n",
    "           s.ANE_since_index,\n",
    "           IF(s.ANE_since_index IS NOT NULL, 1, 0) ANE_ind,\n",
    "           s.ARF_since_index,\n",
    "           IF(s.ARF_since_index IS NOT NULL, 1, 0) ARF_ind,\n",
    "           s.ARDS_since_index,\n",
    "           IF(s.ARDS_since_index IS NOT NULL, 1, 0) ARDS_ind,\n",
    "           s.AFE_since_index,\n",
    "           IF(s.AFE_since_index IS NOT NULL, 1, 0) AFE_ind,\n",
    "           s.CAVF_since_index,\n",
    "           IF(s.CAVF_since_index IS NOT NULL, 1, 0) CAVF_ind,\n",
    "           s.COCR_since_index,\n",
    "           IF(s.COCR_since_index IS NOT NULL, 1, 0) COCR_ind,\n",
    "           s.DIC_since_index,\n",
    "           IF(s.DIC_since_index IS NOT NULL, 1, 0) DIC_ind,\n",
    "           s.ECL_since_index,\n",
    "           IF(s.ECL_since_index IS NOT NULL, 1, 0) ECL_ind,\n",
    "           s.HF_since_index,\n",
    "           IF(s.HF_since_index IS NOT NULL, 1, 0) HF_ind,\n",
    "           s.PCD_since_index,\n",
    "           IF(s.PCD_since_index IS NOT NULL, 1, 0) PCD_ind,\n",
    "           s.PE_since_index,\n",
    "           IF(s.PE_since_index IS NOT NULL, 1, 0) PE_ind,\n",
    "           s.SAC_since_index,\n",
    "           IF(s.SAC_since_index IS NOT NULL, 1, 0) SAC_ind,\n",
    "           s.SEP_since_index,\n",
    "           IF(s.SEP_since_index IS NOT NULL, 1, 0) SEP_ind,\n",
    "           s.SSH_since_index,\n",
    "           IF(s.SSH_since_index IS NOT NULL, 1, 0) SSH_ind,\n",
    "           s.SCC_since_index,\n",
    "           IF(s.SCC_since_index IS NOT NULL, 1, 0) SCC_ind,\n",
    "           s.ATE_since_index,\n",
    "           IF(s.ATE_since_index IS NOT NULL, 1, 0) ATE_ind,\n",
    "           s.BPT_since_index,\n",
    "           IF(s.BPT_since_index IS NOT NULL, 1, 0) BPT_ind,\n",
    "           s.HYS_since_index,\n",
    "           IF(s.HYS_since_index IS NOT NULL, 1, 0) HYS_ind,\n",
    "           s.TT_since_index,\n",
    "           IF(s.TT_since_index IS NOT NULL, 1, 0) TT_ind,\n",
    "           s.VEN_since_index,\n",
    "           IF(s.VEN_since_index IS NOT NULL, 1, 0) VEN_ind\n",
    "    from delivery_elig_tbl1 e\n",
    "    left join smm_any a \n",
    "    on e.person_id = a.person_id and e.event_id = a.event_id\n",
    "    left join smm_post_delivery_wide s \n",
    "    on e.person_id = s.person_id and e.event_id = s.event_id\n",
    "''').cache()\n",
    "delivery_elig_smm.createOrReplaceTempView(\"delivery_elig_smm\")\n",
    "delivery_elig_smm.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# save elig table as parquet file\n",
    "delivery_elig_tbl1.write.save('delivery_elig_tbl1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "delivery_elig_tbl1 =  spark.read.load(\"delivery_elig_tbl1.parquet\")\n",
    "delivery_elig_tbl1.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# def pt_freq_qry(df,stratified_by,n_way=1):\n",
    "#     '''\n",
    "#     generate total patient counts for each stratified variables\n",
    "#     '''\n",
    "#     sql_str_lst = []\n",
    "#     # overall count\n",
    "#     sql_str_lst.append(\"select 'total' as summ_var,'N' as summ_cat, count(distinct person_id) as pat_cnt, count(distinct person_id || '_' || event_id) as evt_cnt from \" + df)\n",
    "    \n",
    "#     # 1-way summary\n",
    "#     for var_str in stratified_by:\n",
    "#         sql_str_lst.append(\n",
    "#             \"select '\" + var_str +\"' as summ_var,\" \n",
    "#             + \"cast(\" + var_str +\" as string) as summ_cat,\"\n",
    "#             + \"count(distinct person_id) as pat_cnt, \"\n",
    "#             + \"count(distinct person_id || '_' || event_id) as evt_cnt \"\n",
    "#             + \"from \"+ df + \" group by \"+ var_str\n",
    "#         )\n",
    "        \n",
    "#     # up to n-way summary\n",
    "#     if n_way > 1:\n",
    "#         for L in range(2,n_way+1,1):\n",
    "#             for var_str_comb in itertools.combinations(stratified_by, L):\n",
    "#                 var_str_concat_by_pipe = \"|\".join(var_str_comb)\n",
    "#                 var_str_concat_by_dpipe = \"|| '||' ||\".join(var_str_comb)\n",
    "#                 var_str_concat_by_comma = \",\".join(var_str_comb)\n",
    "#                 sql_str_lst.append(\n",
    "#                     \"select 'by_\" + var_str_concat_by_pipe +\"' as summ_var,\" \n",
    "#                     + \"cast(\" + var_str_concat_by_dpipe +\" as string) as summ_cat,\"\n",
    "#                     + \"count(distinct person_id) as pat_cnt, \"\n",
    "#                     + \"count(distinct person_id || '_' || event_id) as evt_cnt \"\n",
    "#                     + \"from \"+ df + \" group by \"+ var_str_concat_by_comma\n",
    "#                 )\n",
    "                \n",
    "#     # union everything\n",
    "#     sql_str = \" union \".join(sql_str_lst)\n",
    "#     return(sql_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# stratified_by = [\n",
    "#     'agegrp_at_event',\n",
    "#     'race_source_value',\n",
    "#     'ethnicity_source_value',\n",
    "#     'segment',\n",
    "#     'speciality',\n",
    "#     'bed_size',\n",
    "#     'zip_code',\n",
    "#     'death_ind',\n",
    "#     'SMMANY_ind'\n",
    "# ]\n",
    "# get_pt_summ = pt_freq_qry(\n",
    "#     'delivery_elig_tbl1',stratified_by,n_way=2\n",
    "# )\n",
    "# summ_stat_long = spark.sql(get_pt_summ).toPandas()\n",
    "# summ_stat_long.to_csv(os.path.join(path_to_data,'summ_stat_2way.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_univariate_odds_ratios(\n",
    "    df,             # spark dataframe\n",
    "    covariate_cols, # list of covariates\n",
    "    outcome_cols    # list of outcomes\n",
    "):\n",
    "    odds_ratios = {}\n",
    "    for outcome in outcome_cols:\n",
    "        for covariate in covariate_cols:       \n",
    "            # Determine if the covariate is categorical\n",
    "            if dict(df.dtypes)[covariate] == 'string':\n",
    "                # Index the categorical column\n",
    "                indexer = StringIndexer(inputCol=covariate, outputCol=covariate + \"_indexed\")\n",
    "                indexed_df = indexer.fit(df).transform(df)\n",
    "\n",
    "                # One-hot encode the indexed column\n",
    "                encoder = OneHotEncoder(inputCol=covariate + \"_indexed\", outputCol=covariate + \"_encoded\")\n",
    "                encoded_df = encoder.transform(indexed_df)\n",
    "\n",
    "                # Assemble features for logistic regression\n",
    "                assembler = VectorAssembler(inputCols=[covariate + \"_encoded\"], outputCol=\"features\")\n",
    "                assembled_df = assembler.transform(encoded_df.select(outcome, covariate + \"_encoded\"))\n",
    "            else:\n",
    "                # Assemble features for logistic regression\n",
    "                assembler = VectorAssembler(inputCols=[covariate], outputCol=\"features\")\n",
    "                assembled_df = assembler.transform(df.select(outcome, covariate))\n",
    "\n",
    "            # Fit logistic regression model\n",
    "            lr = LogisticRegression(featuresCol=\"features\", labelCol=outcome)\n",
    "            model = lr.fit(assembled_df)\n",
    "\n",
    "            # Extract coefficients and calculate odds ratios\n",
    "            for i, coef in enumerate(model.coefficients):\n",
    "                odds_ratio = np.exp(coef)\n",
    "                # Collect summary for confidence intervals\n",
    "                summary = model.summary\n",
    "                if hasattr(summary, 'coefficientStandardErrors'):\n",
    "                    coefficient_standard_error = summary.coefficientStandardErrors[i]\n",
    "                else:\n",
    "                    coefficient_standard_error = 0\n",
    "                z_value = 1.96  # for 95% confidence interval\n",
    "                conf_lower = np.exp(coef - z_value * coefficient_standard_error)\n",
    "                conf_upper = np.exp(coef + z_value * coefficient_standard_error)\n",
    "\n",
    "                odds_ratios[f\"{covariate}_{i}\"] = {\n",
    "                    'odds_ratio': odds_ratio,\n",
    "                    'conf_lower': conf_lower,\n",
    "                    'conf_upper': conf_upper\n",
    "                }\n",
    "    return odds_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cov_lst = ['race_source_value']\n",
    "out_lst = ['SMMANY_ind']\n",
    "results = calculate_univariate_odds_ratios(\n",
    "    df = delivery_elig_smm,\n",
    "    covariate_cols = cov_lst,\n",
    "    outcome_cols = out_lst\n",
    ")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
