{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import urllib.request as urlreq\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "path_to_data = os.path.join(os.getcwd(),'University of Missouri','DRIVERS','data')\n",
    "path_to_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"use real_world_data_ed_omop_dec_2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cd_meta = spark.sql('''\n",
    "    select concept_id,concept_name,concept_code,vocabulary_id,domain_id\n",
    "    from concept\n",
    "    where vocabulary_id = 'DRG' and\n",
    "          concept_code in (\n",
    "            '765','766','767','768',\n",
    "            '774','775',\n",
    "            '783','784','785','786', '787','788',\n",
    "            '796','797','798',\n",
    "            '805','806','807'\n",
    "          )\n",
    "    union all \n",
    "    select concept_id,concept_name,concept_code,vocabulary_id,domain_id\n",
    "    from concept\n",
    "    where vocabulary_id = 'CPT4' and\n",
    "          concept_code in (\n",
    "            '59409','59514', '59612','59620'\n",
    "          )\n",
    "    union all \n",
    "    select concept_id,concept_name,concept_code,vocabulary_id,domain_id\n",
    "    from concept\n",
    "    where vocabulary_id = 'ICD10PCS' and\n",
    "          concept_code in (\n",
    "            '10D00Z0','10D00Z1','10D00Z2','10D07Z3','10D07Z4', '10D07Z5', '10D07Z6','10D07Z7','10D07Z8',\n",
    "            '10E0XZZ'\n",
    "          )\n",
    "    union all\n",
    "    select concept_id,concept_name,concept_code,vocabulary_id,domain_id\n",
    "    from concept\n",
    "    where vocabulary_id = 'LOINC' and\n",
    "          concept_code in (\n",
    "            \n",
    "          )\n",
    "''').toPandas()\n",
    "cd_meta.to_csv(os.path.join(path_to_data,'cd_meta_omop.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def cd_where_clause(meta_tbl,voc_id):\n",
    "    '''\n",
    "    generate where clause details based on omop concept_id\n",
    "    '''\n",
    "    cd_lst = meta_tbl.loc[meta_tbl['vocabulary_id']==voc_id,'concept_id'].tolist()\n",
    "    cd_quote = []\n",
    "    for code in cd_lst:\n",
    "        cd_quote.append(\"'\"+ str(code) +\"'\")\n",
    "    cd_quote_str = \",\".join(cd_quote)\n",
    "    return cd_quote_str     \n",
    "\n",
    "meta_tbl = pd.read_csv(os.path.join(path_to_data,'cd_meta_omop.csv'))\n",
    "drg_where = cd_where_clause(meta_tbl,'DRG')\n",
    "cpt4_where = cd_where_clause(meta_tbl,'CPT4')\n",
    "icd10_where = cd_where_clause(meta_tbl,'ICD10PCS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "delivery_init = spark.sql('''\n",
    "    select person_id,\n",
    "           visit_occurrence_id, \n",
    "           visit_detail_id, \n",
    "           observation_date as event_date, \n",
    "           observation_concept_id as event_identifier,\n",
    "           'DRG' as event_source\n",
    "    from observation\n",
    "    where observation_concept_id in ('''+ drg_where +''')\n",
    "    union all\n",
    "    select person_id,\n",
    "           visit_occurrence_id, \n",
    "           visit_detail_id, \n",
    "           procedure_date as event_date, \n",
    "           procedure_concept_id as event_identifier,\n",
    "           'CPT4' as event_source\n",
    "    from procedure_occurrence\n",
    "    where procedure_concept_id in ('''+ cpt4_where +''')\n",
    "    union all\n",
    "    select person_id,\n",
    "           visit_occurrence_id, \n",
    "           visit_detail_id, \n",
    "           procedure_date as event_date, \n",
    "           procedure_concept_id as event_identifier,\n",
    "           'ICD10PCS' as event_source\n",
    "    from procedure_occurrence\n",
    "    where procedure_concept_id in ('''+ icd10_where +''')\n",
    "''').cache()\n",
    "delivery_init.createOrReplaceTempView(\"delivery_init\")\n",
    "delivery_init.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "    select * from delivery_init\n",
    "    where visit_occurrence_id = '11888469770784'\n",
    "    limit 5\n",
    "''').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "delivery_ip = spark.sql('''\n",
    "    select distinct\n",
    "           a.person_id, \n",
    "           a.visit_occurrence_id,\n",
    "           v.visit_start_date,\n",
    "           v.visit_end_date,\n",
    "           v.care_site_id\n",
    "    from delivery_init a \n",
    "    join visit_occurrence v \n",
    "    on a.person_id = v.person_id and \n",
    "       a.visit_occurrence_id = v.visit_occurrence_id\n",
    "    where v.visit_concept_id in (\n",
    "        9201, -- IP\n",
    "        262   -- ER to IP\n",
    "    )\n",
    "''').cache()\n",
    "delivery_ip.createOrReplaceTempView(\"delivery_ip\")\n",
    "delivery_ip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "delivery_consolidate = spark.sql('''\n",
    "    with cd_filter as (\n",
    "        select v.person_id, \n",
    "               v.visit_occurrence_id,\n",
    "               v.care_site_id,\n",
    "               v.visit_start_date,\n",
    "               v.visit_end_date,\n",
    "               a.event_source,\n",
    "               a.event_date,\n",
    "               row_number() over (partition by v.person_id, v.visit_occurrence_id, a.event_source order by a.event_date) as rn_asc,\n",
    "               row_number() over (partition by v.person_id, v.visit_occurrence_id, a.event_source order by a.event_date desc) as rn_desc\n",
    "        from delivery_ip v\n",
    "        join delivery_init a \n",
    "        on v.person_id = a.person_id and \n",
    "           v.visit_occurrence_id = a.visit_occurrence_id\n",
    "        where a.event_date between date_sub(v.visit_start_date,3) and date_add(v.visit_end_date,3)\n",
    "    ), f_pvt as (\n",
    "        select * \n",
    "        from (\n",
    "            select person_id, visit_occurrence_id,\n",
    "                   event_source, event_date\n",
    "            from cd_filter\n",
    "            where rn_asc = 1       \n",
    "        )\n",
    "        pivot (\n",
    "            min(event_date) for event_source in (\n",
    "                'DRG' as F_DRG_DT,'CPT4' as F_CPT_DT,'ICD10PCS' as F_ICD_DT\n",
    "            )\n",
    "        )\n",
    "    ), l_pvt as (\n",
    "        select * \n",
    "        from (\n",
    "            select person_id, visit_occurrence_id,\n",
    "                   event_source, event_date\n",
    "            from cd_filter\n",
    "            where rn_desc = 1       \n",
    "        )\n",
    "        pivot (\n",
    "            max(event_date) for event_source in (\n",
    "                'DRG' as L_DRG_DT,'CPT4' as L_CPT_DT,'ICD10PCS' as L_ICD_DT\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    select a.person_id, \n",
    "           a.visit_occurrence_id,\n",
    "           a.visit_start_date,\n",
    "           a.visit_end_date,\n",
    "           a.care_site_id,\n",
    "           f.F_DRG_DT,f.F_CPT_DT,f.F_ICD_DT,\n",
    "           l.L_DRG_DT,l.L_CPT_DT,l.L_ICD_DT\n",
    "    from delivery_ip a \n",
    "    left join f_pvt f on a.person_id = f.person_id and a.visit_occurrence_id = f.visit_occurrence_id\n",
    "    left join l_pvt l on a.person_id = l.person_id and a.visit_occurrence_id = l.visit_occurrence_id\n",
    "''').cache()\n",
    "delivery_consolidate.createOrReplaceTempView(\"delivery_consolidate\")\n",
    "delivery_consolidate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "delivery_elig = spark.sql('''\n",
    "    with date_consolid as (\n",
    "        select distinct \n",
    "               person_id,\n",
    "               visit_occurrence_id,\n",
    "               care_site_id,\n",
    "               coalesce(F_DRG_DT,visit_start_date,F_ICD_DT,F_CPT_DT) as event_start_dt,\n",
    "               coalesce(L_DRG_DT,visit_end_date,L_ICD_DT,L_CPT_DT) as event_end_dt\n",
    "        from delivery_consolidate    \n",
    "    ), visit_diffs as (\n",
    "        select a.*, \n",
    "               lead(a.event_start_dt, 1, '9999-12-31') OVER (PARTITION BY person_id ORDER BY event_start_dt) AS next_event_start_dt\n",
    "        from date_consolid a \n",
    "    ), visit_session as (\n",
    "        select b.*, \n",
    "               case \n",
    "                   when datediff(b.next_event_start_dt,b.event_start_dt) > 211 then 1\n",
    "                   else 0 \n",
    "               end as new_session_flag\n",
    "        from visit_diffs b\n",
    "    ), sessions as (\n",
    "        select d.*, \n",
    "               sum(d.new_session_flag) over (PARTITION BY d.person_id ORDER BY d.event_start_dt) as event_id\n",
    "        from visit_session d\n",
    "    ), session_order as (\n",
    "        select e.*, \n",
    "               row_number() over (partition by e.person_id, e.event_id order by e.event_start_dt) as rn,\n",
    "               max(e.event_end_dt) over (partition by e.person_id, e.event_id) as event_end_date\n",
    "    from sessions e\n",
    "    )\n",
    "    select s.person_id, \n",
    "           s.event_id, \n",
    "           s.visit_occurrence_id,\n",
    "           cs.care_site_source_value,\n",
    "           s.event_start_dt as event_start_date,\n",
    "           s.event_end_date\n",
    "    from session_order s \n",
    "    join care_site cs on s.care_site_id = cs.care_site_id\n",
    "    where s.rn = 1\n",
    "    order by s.person_id, s.event_id\n",
    "''').cache()\n",
    "delivery_elig.createOrReplaceTempView(\"delivery_elig\")\n",
    "delivery_elig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# delivery_elig_df = spark.createDataFrame(delivery_elig.toPandas())\n",
    "# delivery_elig_df.write.parquet(os.path.join(path_to_data,'delivery_elig.parquet'),mode=\"overwrite\")\n",
    "# permission denied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "delivery_elig_tbl1 = spark.sql('''\n",
    "    select d.person_id,\n",
    "           d.event_id,\n",
    "           d.event_start_date, \n",
    "           d.event_end_date,\n",
    "           d.visit_occurrence_id,\n",
    "           p.year_of_birth,\n",
    "           p.month_of_birth,\n",
    "           p.day_of_birth,\n",
    "           p.race_source_value,\n",
    "           p.ethnicity_source_value,\n",
    "           p.location_id,\n",
    "           p.care_site_id,\n",
    "           d.care_site_source_value,\n",
    "           tnt.bed_size,\n",
    "           tnt.speciality,\n",
    "           tnt.segment,\n",
    "           tnt.zip_code,\n",
    "           dth.death_date,\n",
    "           case when dth.death_date is not null then 1 else 0 end as death_ind\n",
    "    from delivery_elig d\n",
    "    join person p on d.person_id = p.person_id\n",
    "    left join tenant_attributes tnt on d.care_site_source_value = tnt.tenant\n",
    "    left join death dth on d.person_id = dth.person_id \n",
    "''').cache()\n",
    "delivery_elig_tbl1.createOrReplaceTempView(\"delivery_elig_tbl1\")\n",
    "delivery_elig_tbl1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def pt_freq_qry(df,stratified_by,n_way=1):\n",
    "    '''\n",
    "    generate total patient counts for each stratified variables\n",
    "    '''\n",
    "    sql_str_lst = []\n",
    "    # overall count\n",
    "    sql_str_lst.append(\"select 'total' as summ_var,'N' as summ_cat, count(distinct person_id) as pat_cnt, count(distinct person_id || '_' || event_id) as evt_cnt from \" + df)\n",
    "    \n",
    "    # 1-way summary\n",
    "    for var_str in stratified_by:\n",
    "        sql_str_lst.append(\n",
    "            \"select '\" + var_str +\"' as summ_var,\" \n",
    "            + \"cast(\" + var_str +\" as string) as summ_cat,\"\n",
    "            + \"count(distinct person_id) as pat_cnt, \"\n",
    "            + \"count(distinct person_id || '_' || event_id) as evt_cnt \"\n",
    "            + \"from \"+ df + \" group by \"+ var_str\n",
    "        )\n",
    "        \n",
    "    # up to n-way summary\n",
    "    if n_way > 1:\n",
    "        for L in range(2,n_way+1,1):\n",
    "            for var_str_comb in itertools.combinations(stratified_by, L):\n",
    "                var_str_concat_by_pipe = \"|\".join(var_str_comb)\n",
    "                var_str_concat_by_dpipe = \"|| '||' ||\".join(var_str_comb)\n",
    "                var_str_concat_by_comma = \",\".join(var_str_comb)\n",
    "                sql_str_lst.append(\n",
    "                    \"select 'by_\" + var_str_concat_by_pipe +\"' as summ_var,\" \n",
    "                    + \"cast(\" + var_str_concat_by_dpipe +\" as string) as summ_cat,\"\n",
    "                    + \"count(distinct person_id) as pat_cnt, \"\n",
    "                    + \"count(distinct person_id || '_' || event_id) as evt_cnt \"\n",
    "                    + \"from \"+ df + \" group by \"+ var_str_concat_by_comma\n",
    "                )\n",
    "                \n",
    "    # union everything\n",
    "    sql_str = \" union \".join(sql_str_lst)\n",
    "    return(sql_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "stratified_by = [\n",
    "    'race_source_value',\n",
    "    'ethnicity_source_value',\n",
    "    'segment',\n",
    "    'speciality',\n",
    "    'bed_size',\n",
    "    'death_ind',\n",
    "    'zip_code'\n",
    "]\n",
    "get_pt_summ = pt_freq_qry(\n",
    "    'delivery_elig_tbl1',stratified_by,n_way=3\n",
    ")\n",
    "summ_stat_long = spark.sql(get_pt_summ).toPandas()\n",
    "summ_stat_long.to_csv(os.path.join(path_to_data,'summ_stat.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "summ_stat_long = pd.read_csv(os.path.join(path_to_data,'summ_stat.csv'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
